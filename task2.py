# в этой задаче я позволю себе прибегнуть к модулям requests и BeautifulSoup, которые я уже использовал при создании своего парсера
# они показали свою эффективность, и как только я увидел условие, понял, что они будут полезны
# подробно распишу ход действий в комментариях

# по ходу выполнения тестового возник вопрос - насколько приемлемо в рамках него игнорировать то, что есть не только животные, но и семейства, например?
# ведь не напишем же мы "Перламутровые Абботины 77" по отношению к одному человеку.
# чтобы отфильтровать все, потребуется немало спецификаций, но раз требуется получить просто количество, буду считать все.
# если я неправильно понял условие, то могу исправить!

import requests
import re
from bs4 import BeautifulSoup

# копирую буквы с той же страницы в вики, перевожу в список
copypaste = 'А Б В Г Д Е Ё Ж З И Й К Л М Н О П Р С Т У Ф Х Ц Ч Ш Щ Э Ю Я A B C D E F G H I J K L M N O P Q R S T U V W X Y Z'
alphabet = copypaste.split(' ')

url = 'https://ru.wikipedia.org/wiki/%D0%9A%D0%B0%D1%82%D0%B5%D0%B3%D0%BE%D1%80%D0%B8%D1%8F:%D0%96%D0%B8%D0%B2%D0%BE%D1%82%D0%BD%D1%8B%D0%B5_%D0%BF%D0%BE_%D0%B0%D0%BB%D1%84%D0%B0%D0%B2%D0%B8%D1%82%D1%83'

def listofanimals(names, url):
    # переменная soup - вся разметка страницы
    soup = BeautifulSoup(requests.get(url).text, 'lxml')
    # получаем ветку mw-pages, в котором мы будем находить названия и в котором содержится ссылка на след.страницу
    list = soup.find('div', attrs={'id': 'mw-pages'})
    # переменная find - все ссылки на страницы в рамках этой ветки, это нам и нужно
    find = list.find_all('li')
    # в этом цикле я обращаюсь к тегу a, внутри которого я и найду названия по атрибуту 'title', сразу добавляю их в список
    for i in find:
        a = i.find_all('a')
        for aa in a:
            names.append(aa.get('title'))

    #обработка исключений: рано или поздно надпись "Следующая страница" станет некликабельной
    try:
        next = list.find('a', string=re.compile('Следующая страница')).get('href')
        listofanimals(names, f'https://ru.wikipedia.org{next}')
    except AttributeError:
        pass
    return(names)

#передаю в функцию заготовку для списка, ну и саму ссылку
test = listofanimals([], url)

def search(test, left, right, dict):
    # работаю со списком, все тот же бинарный поиск
    for i in alphabet:
        while not right - left == 1:
            mid = int((left + right) / 2)
            if test[mid].startswith(i):
                left = mid
            else:
                right = mid
        # нахожу крайний правый индекс слов на нужную букву
        # допустим, на букву А мы нашли 1089 названий
        # правый индекс как раз будет под номером 1089, добавляем в словарь
        # следующий проход цикла начнем с него, т.е. с 1090-го элемента
        dict[i] = right
        # произвожу срез и иду дальше по циклу
        test = test[right:]
        # после среза крайние индексы - к исходным значениям
        left = 0
        right = len(test) - 1

    #return dict - мы можем на этом остановиться, и потом через print вызвать функцию, но раз уж нужно в более красивом виде вывод...
    for i in dict.keys():
        print (f'{i}: {dict[i]}')

search(test, 0, len(test) - 1, {})
#print(search(test, 0, len(test) - 1, {}))

# как проверить достоверность полученных данных? тут мне бы пришлось серьезно поломать голову...
# но вручную я проверил, что животных на букву А - 1089, как и высчитал мой код, а на Ё и Й - по 3 штуки.
# что необходимо доработать? например, животные на букву Ё затерялись между животными на букву Е
# то есть в силу особенностей кода в статистику не попали 4 страницы с животными на Е
# как проработать эту проблему, учитывать ее заранее? тут мне также стоило бы серьезно поломать голову...
